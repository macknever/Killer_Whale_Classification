{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import cv2\n",
    "from skimage import io, transform\n",
    "import pickle\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDict(dict):\n",
    "        def __init__(self, *args):\n",
    "            super(DeviceDict, self).__init__(*args)\n",
    "\n",
    "        def to(self, device):\n",
    "            dd = DeviceDict()\n",
    "            for k, v in self.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    dd[k] = v.to(device)\n",
    "                else:\n",
    "                    dd[k] = v\n",
    "            return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = [\"resident\", \"transient\"]\n",
    "\n",
    "class Killer_Whale_Dataset(Dataset):\n",
    "    def __init__(self, data_folder, transform = None):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        if os.path.exists(\"data/data.npy\"):\n",
    "            self.data = np.load(\"data/data.npy\", allow_pickle=True)\n",
    "        else:\n",
    "            self.img_list, self.mask_list = Killer_Whale_Dataset._load_dataset(data_folder)\n",
    "            self.img_list.sort(key=lambda x: x[\"path\"])\n",
    "            self.mask_list.sort(key=lambda x: x[\"path\"])\n",
    "            self.img_list, self.mask_list = Killer_Whale_Dataset._remove_grayscale(self.img_list, self.mask_list)\n",
    "            self.data = Killer_Whale_Dataset.merge_masks_and_imgs(self.img_list, self.mask_list)\n",
    "            random.shuffle(self.data)\n",
    "            np.save(\"data/data.npy\", self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        elt = self.data[idx]\n",
    "\n",
    "        img_array = elt[\"img\"]\n",
    "        species_array = elt[\"species\"]\n",
    "        mask_array = elt[\"mask\"]\n",
    "        if self.transform:\n",
    "            img_array = self.transform(img_array)\n",
    "            mask_array = self.transform(mask_array)\n",
    "       \n",
    "        return DeviceDict({\"img\": img_array, \n",
    "                \"id\": elt[\"id\"], \n",
    "                \"species\": species_array,\n",
    "                \"mask\": mask_array}) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    @staticmethod\n",
    "    def _remove_grayscale(img_list, mask_list):\n",
    "        indices = []\n",
    "        for i, img in enumerate(img_list):\n",
    "            num_channels = img[\"img\"].mode\n",
    "            if num_channels != \"RGB\":\n",
    "                indices.append(i)\n",
    "        img_list = np.delete(img_list, indices)\n",
    "        mask_list = np.delete(mask_list, indices)\n",
    "        return img_list, mask_list\n",
    "\n",
    "    @staticmethod \n",
    "    def _load_dataset(path):\n",
    "        imgs = []\n",
    "        masks = []\n",
    "        for subdir, dirs, files in os.walk(path):\n",
    "            for f in files:\n",
    "                whale = {}\n",
    "                filepath = subdir + \"/\" + f\n",
    "                #print(filepath)\n",
    "                # There is no mask for picture KW6CA171B_6.jpg so skip it for now\n",
    "                if \"KW6CA171B_6.jpg\" in filepath:\n",
    "                    continue\n",
    "                \n",
    "                id = torch.tensor(float(filepath.split(\"/\")[2][-3:]))\n",
    "                species =  filepath.split(\"/\")[1]\n",
    "                img_or_mask = filepath.split(\"/\")[3]\n",
    "            \n",
    "                whale[\"id\"] = id\n",
    "                whale[\"path\"] = filepath\n",
    "\n",
    "                if \"resident\" in species:\n",
    "                    whale[\"species\"] = 0\n",
    "                elif \"transient\" in species:\n",
    "                    whale[\"species\"] = 1\n",
    "\n",
    "                img = Image.open(filepath)\n",
    "                #img = img.resize((400, 400))\n",
    "                img = img.resize((224, 224)) \n",
    "                if img_or_mask == \"img\" or img_or_mask == \"IMG\":\n",
    "                    whale[\"img\"] = img \n",
    "                    imgs.append(whale)\n",
    "                elif img_or_mask == \"mask\":\n",
    "                    whale[\"mask\"] = img\n",
    "                    masks.append(whale)\n",
    "        \n",
    "        return imgs, masks \n",
    "\n",
    "    @staticmethod\n",
    "    def merge_masks_and_imgs(img_list, mask_list):\n",
    "        data = []\n",
    "        for i, val in enumerate(img_list):\n",
    "            mask = mask_list[i][\"mask\"]\n",
    "            val[\"mask\"] = mask\n",
    "            data.append(val)\n",
    "        return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# whale_path = 'data/'\n",
    "\n",
    "# whale_data = Killer_Whale_Dataset(whale_path, transform=transform)\n",
    "# #print(whale_data.__len__())\n",
    "# print(type(whale_data[0]['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tensor2Image(t):\n",
    "    trans = transforms.ToPILImage()\n",
    "    img = trans(t[0,:,:,:])\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## this is for 600*600 size image\n",
    "        #self.conv2a = nn.Conv2d(in_channels=3, out_channels=128,kernel_size=#,stride = 3)\n",
    "        self.conv2a = nn.Conv2d(in_channels=3, out_channels=80,kernel_size=4,stride = 3)\n",
    "        \n",
    "        self.conv2b = nn.Conv2d(in_channels=80, out_channels=80,kernel_size=4,stride = 2)\n",
    "\n",
    "        # self.conv2c = nn.Conv2d(in_channels=128, out_channels=128,kernel_size=3,stride = 2)\n",
    "        # self.conv2d = nn.Conv2d(in_channels=128, out_channels=128,kernel_size=3,stride = 2)\n",
    "        # self.conv2e = nn.Conv2d(in_channels=128, out_channels=128,kernel_size=3,stride = 2)\n",
    "        # self.conv2f = nn.Conv2d(in_channels=128, out_channels=128,kernel_size=3,stride = 2)\n",
    "        self.pool = nn.MaxPool2d(3, 3)\n",
    "        \n",
    "        ## this is for 600*600 size image    \n",
    "        #self.fc1a = nn.Linear(128*10*10,80)\n",
    "        ## this is for 224*224 size image\n",
    "        self.fc1a = nn.Linear(80*3*3,80)\n",
    "        self.fc1b = nn.Linear(80,40)\n",
    "\n",
    "        self.fc2a = nn.Linear(40,80)\n",
    "        self.fc2b = nn.Linear(80,80*11*11)\n",
    "        #self.fc2c = nn.Linear(128,3*224*224)\n",
    "        self.convtrans2a = nn.ConvTranspose2d(in_channels=80, out_channels=80,kernel_size=4,stride = 2)\n",
    "        self.convtrans2b = nn.ConvTranspose2d(in_channels=80, out_channels=80,kernel_size=4,stride = 3)\n",
    "        self.convtrans2c = nn.ConvTranspose2d(in_channels=80, out_channels=3,kernel_size=8,stride = 3)\n",
    "        self.convtrans2d = nn.ConvTranspose2d(in_channels=128, out_channels=128,kernel_size=3,stride = 2)\n",
    "        self.convtrans2e = nn.ConvTranspose2d(in_channels=128, out_channels=128,kernel_size=3,stride = 2)\n",
    "        self.convtrans2f = nn.ConvTranspose2d(in_channels=128, out_channels=128,kernel_size=3,stride = 2)\n",
    "        self.convtrans2g = nn.ConvTranspose2d(in_channels=128, out_channels=3,kernel_size=4,stride = 2)\n",
    "        \n",
    "        ## Here, we should define some smart layers\n",
    "    def encode(self, dictionary):\n",
    "        ## Use Deep NN to encode the image\n",
    "\n",
    "        x = dictionary['img']\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        e1 = nn.ReLU()(self.conv2a(x))\n",
    "        j1 = self.pool(e1)\n",
    "        e2 = nn.ReLU()(self.conv2b(j1))\n",
    "        j2 = self.pool(e2)\n",
    "        # e3 = nn.ReLU()(self.conv2c(e2))\n",
    "        # e4 = nn.ReLU()(self.conv2d(e3))\n",
    "        # e5 = nn.ReLU()(self.conv2e(e4))\n",
    "        # e6 = nn.ReLU()(self.conv2f(e5))\n",
    "        \n",
    "        j2 = self.pool(e2)\n",
    "        \n",
    "        #print(e2.size())\n",
    "        k1 = j2.view(batch_size,-1)\n",
    "        k2 = nn.ReLU()(self.fc1a(k1))\n",
    "        latent_info = self.fc1b(k2)\n",
    "        return latent_info\n",
    "    \n",
    "    def decode(self, latent_info):\n",
    "\n",
    "\n",
    "        ## use the NN to decode to mask\n",
    "        batch_size = latent_info.shape[0]\n",
    "        \n",
    "        h1 = nn.ReLU()(self.fc2a(latent_info))\n",
    "        h2 = nn.ReLU()(self.fc2b(h1))\n",
    "        # h3 = self.fc2c(h2)\n",
    "        \n",
    "        ## this is for 600*600 size image       \n",
    "        #y = h3.view(batch_size,-1,600,600)\n",
    "        \n",
    "        y = h2.view(batch_size,-1,11,11)\n",
    "\n",
    "        d1 = nn.ReLU()(self.convtrans2a(y))\n",
    "        \n",
    "        d2 = nn.ReLU()(self.convtrans2b(d1))\n",
    "        d3 = self.convtrans2c(d2)\n",
    "        #print(d3.size())\n",
    "        # d3 = nn.ReLU()(self.convtrans2c(d2))\n",
    "        \n",
    "        # d4 = nn.ReLU()(self.convtrans2d(d3))\n",
    "        # d5 = nn.ReLU()(self.convtrans2e(d4))\n",
    "        # d6 = nn.ReLU()(self.convtrans2f(d5))\n",
    "        # d7 = self.convtrans2g(d6)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return {'img': d3}\n",
    "\n",
    "    def forward(self, dictionary):\n",
    "        latent_info = self.encode(dictionary)        \n",
    "        poly_dict = self.decode(latent_info)\n",
    "        return poly_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6cb156891bdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mwhale_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKiller_Whale_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhale_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhale_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-86e588e0d73e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_folder, transform)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKiller_Whale_Dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_masks_and_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/data.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/data.npy'"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])   \n",
    "\n",
    "whale_path = 'data/'\n",
    "\n",
    "\n",
    "whale_data = Killer_Whale_Dataset(whale_path,transform = transform)\n",
    "\n",
    "data_len = len(whale_data)\n",
    "#data_len = int(0.1*data_len)\n",
    "val_len = int(0.1*data_len)\n",
    "x = list(range(0,data_len))\n",
    "ind_val = random.choices(x, k=val_len)\n",
    "ind_train = list(set(x).difference(set(ind_val)))\n",
    "\n",
    "\n",
    "\n",
    "val_set = torch.utils.data.Subset(whale_data,ind_val)\n",
    "train_set = torch.utils.data.Subset(whale_data,ind_train)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size = 5,shuffle = True,drop_last = False)\n",
    "val_loader = torch.utils.data.DataLoader(val_set,batch_size = 1,shuffle = True,drop_last = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Orca_Classify(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Orca_Classify, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(40 , 100)\n",
    "        self.fc2 = nn.Linear(100, 40)\n",
    "        self.fc3 = nn.Linear(40,1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, latent_info):\n",
    "        #print(latent_info.size())\n",
    "        batch_size = latent_info.shape[0]\n",
    "        x = nn.ReLU()(self.fc1(latent_info))\n",
    "        x = nn.ReLU()(self.fc2(x))\n",
    "        #x = nn.LeakyReLU()(self.fc3(x))\n",
    "        #x = x.view(batch_size,-1)\n",
    "        #x = x.squeeze()\n",
    "        #x = self.softmax(x)\n",
    "        #print(x)\n",
    "        return {\"id\": x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './para/weight/para_224_adam.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-006cbf1faf42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdetection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./para/weight/para_224_adam.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mClassification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrca_Classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './para/weight/para_224_adam.pt'"
     ]
    }
   ],
   "source": [
    "detection = AE()\n",
    "detection.load_state_dict(torch.load('./para/weight/para_224_adam.pt'))\n",
    "\n",
    "Classification = Orca_Classify()\n",
    "\n",
    "optimizer = torch.optim.Adam(Classification.parameters(), lr=0.001)\n",
    "#optimizer = torch.optim.SGD(detection.parameters(), lr=0.01,momentum = 0.9)\n",
    "losses = []\n",
    "losses_mean=[]\n",
    "valError = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2d1f76fc65c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython import display\n",
    "import statistics\n",
    "fig=plt.figure(figsize=(40, 10), dpi= 40, facecolor='w', edgecolor='k')\n",
    "axes=fig.subplots(1,2)\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_iter = iter(train_loader)\n",
    "    for i in range(len(train_loader)):\n",
    "        batch = next(train_iter)\n",
    "        #print(batch['id'].unsqueeze(1))\n",
    "        preds = Classification(detection.encode(batch))\n",
    "        \n",
    "        loss = nn.functional.cross_entropy(preds['id'], batch['id'].long()-101)\n",
    "        #loss = nn.MSELoss()(preds['id'], (batch['id']-101))\n",
    "#         print(torch.round(preds['id']).argmax(1)+101)\n",
    "        #print(batch['id'])\n",
    "        #print(preds['id']+101)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(network.module.conv1.weight.grad)\n",
    "        losses.append(loss.item())\n",
    "        losses_mean.append(statistics.mean(losses))\n",
    "        torch.save(Classification.state_dict(),'./para/weight/para_classify3.pt')\n",
    "        if i % 10 == 0:\n",
    "          for ax in axes:\n",
    "            ax.cla() \n",
    "            \n",
    "            \n",
    "            axes[0].plot(losses)\n",
    "            axes[0].set_yscale('log')\n",
    "            \n",
    "            axes[0].set_title('Mean loss') \n",
    "            axes[1].plot(losses_mean)\n",
    "            axes[0].set_yscale('log')\n",
    "            \n",
    "            axes[0].set_title('Training loss')\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            print(\"Plot after epoch {} (iteration {})\".format(epoch, len(losses))) \n",
    "    \n",
    "    #val_accuracy = get_error(network, validation_loader)\n",
    "    #valError.append(val_accuracy)\n",
    "   # print(\"Val Accuracy: \" + str(val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _contains(a,b):\n",
    "    correct = 0\n",
    "    if a.shape[0] == b.shape[0]:\n",
    "        batch_size = a.shape[0]\n",
    "        for i in range(batch_size):\n",
    "            correct += a[i] in b[i,:]\n",
    "    else:\n",
    "        print('size not match')\n",
    "    return correct\n",
    "            \n",
    "\n",
    "def get_error_topk(classifier,detector,data,k):\n",
    "    iterator = iter(data)\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    for i in range(len(data)):\n",
    "        batch = next(iterator)\n",
    "        \n",
    "        preds = classifier(detector.encode(batch))\n",
    "        \n",
    "        correct = _contains(batch['id']-101,torch.topk(preds['id'],k)[1])\n",
    "        total += len(batch['id'])\n",
    "        acc += correct\n",
    "    return acc / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.4230769230769231\n"
     ]
    }
   ],
   "source": [
    "val_accuracy = get_error_topk(Classification,detection,val_loader,10)\n",
    "valError.append(val_accuracy)\n",
    "print(\"Val Accuracy: \" + str(val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
